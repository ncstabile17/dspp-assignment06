---
title: "Assignment 07"
author: "Nick Stabile"
date: "11/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(tidymodels)
library(vip)
```

## Exercise 01

### Creating the data set

```{r}

# use this url to download the data directly into R
df <- read_csv("https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv")

# clean names with janitor
sampled_df <- df %>% 
  janitor::clean_names() 

# create an inspection year variable
sampled_df <- sampled_df %>%
  mutate(inspection_date = mdy(inspection_date)) %>%
  mutate(inspection_year = year(inspection_date))

# get most-recent inspection
sampled_df <- sampled_df %>%
  group_by(camis) %>%
  filter(inspection_date == max(inspection_date)) %>%
  ungroup()

# subset the data
sampled_df <- sampled_df %>%
  select(camis, boro, zipcode, cuisine_description, inspection_date,
         action, violation_code, violation_description, grade,
         inspection_type, latitude, longitude, council_district,
         census_tract, inspection_year, critical_flag) %>%
  filter(complete.cases(.)) %>%
  filter(inspection_year >= 2017) %>%
  filter(grade %in% c("A", "B", "C")) 

# create the binary target variable
sampled_df <- sampled_df %>%
  mutate(grade = if_else(grade == "A", "A", "Not A")) %>%
  mutate(grade = as.factor(grade))

# create extra predictors
sampled_df <- sampled_df %>%
  group_by(boro, zipcode, cuisine_description, inspection_date,
           action, violation_code, violation_description, grade,
           inspection_type, latitude, longitude, council_district,
           census_tract, inspection_year)  %>%
  mutate(vermin = str_detect(violation_description, pattern = "mice|rats|vermin|roaches")) %>%
  summarize(violations = n(),
            vermin_types = sum(vermin),
            critical_flags = sum(critical_flag == "Y")) %>%
  ungroup()

# write the data
write_csv(sampled_df, "restaurant_grades.csv")


```

### 1. Estimate a model

```{r}
set.seed(20201020)

# create a split object
restaurant_grades_split <- initial_split(data = sampled_df, prop = 0.8)

# create the training and testing data
restaurant_grades_train <- training(x = restaurant_grades_split)
restaurant_grades_test <- testing(x = restaurant_grades_split)

# create recipe and downsample grades
predict_grade_cart_recipe <-
  recipe(formula = grade ~ ., data = restaurant_grades_train) %>%
  themis::step_downsample(grade)

# create model object
predict_grade_cart_mod <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

# create a workflow with the recipe
predict_grade_cart_workflow <-
  workflow() %>%
  add_model(spec = predict_grade_cart_mod) %>%
  add_recipe(recipe = predict_grade_cart_recipe)

# fit the model using the workflow
predict_grade_cart_fit <- 
  predict_grade_cart_workflow %>% 
  fit(data = restaurant_grades_train)

```

### 2. Evaluate the model

```{r}

# use the model to get the predictions on the test data
grade_predictions <- bind_cols(
  restaurant_grades_test,
  predict(object = predict_grade_cart_fit, new_data = restaurant_grades_test)
)

# create a confusion matrix based on predictions of test data
grade_pred_conf_matrix <- table(grade_predictions$.pred_class, grade_predictions$grade)

grade_pred_conf_matrix

# Calculate precision and recall/sensitivity "by hand"
grade_pred_precision <- grade_pred_conf_matrix[1]/(grade_pred_conf_matrix[1] + grade_pred_conf_matrix[3])
grade_pred_precision

grade_pred_recall <- grade_pred_conf_matrix[1]/(grade_pred_conf_matrix[1] + grade_pred_conf_matrix[2])
grade_pred_recall

# Calculate precision and recall/sensitivity with tidymodels
precision(data = grade_predictions, truth = grade, estimate = .pred_class)

recall(data = grade_predictions, truth = grade, estimate = .pred_class)

```
c. The precision and recall calculated "by hand" match the values for precision and recall calculated using the tidymodels functions.

d. The model is very precise, with over 99% precision, meaning that it's almost almost correct when it predicts a restaurant will have an A grade. The model does somewhat worse on recall, with around 81% sensitivity, meaning that when the grade should be an A, there is about a 20% chance the model will predict Not A. In this sense it is a somewhat conservative model: it does not predict an A grade when it shouldn't (false positive), but it occasionally fails to predict A when it should have (false negative). This is probably a good thing for restaurant patrons since it is overly protective of their health, but would harm the marginal restaurant owner who should be receiving an A grade, but is not.

### 3. Improvement

The model is quite precise, but could at least improve its recall (among other metrics) so we would want to make improvements that would ideally reduce false negatives by providing more information to correctly classify restaurants that should've been given an A grade but were not. More information about a restaurant owner's history could be potentially useful: grades from previous years or grades from additional or previous restaurants may be helpful in predicting the current restaurant grade. Additionally, having more specific and descriptive location information (i.e. combining lat/lon to determine relative distance) could help in predicting pockets of restaurants that may receive failing grades because of environmental conditions like the presence of vermin. Experimenting with different algorithms could also be a source of improvement including using a random forest algorithm, which is typically more accurate and still slightly interpretable.

### 4. Variable Importance

```{r}
predict_grade_cart_fit %>% 
  pull_workflow_fit() %>%
  vip(num_features = 10)
```

In CART decision tree models, for each split in the tree the model picks the most efficient predictor or variable to split the data, or the one that reduces misclassification the most/improves the model the most as compared to all other options. Calculating variable importance then requires comparing how often variables are used to perform that split and how relatively valuable that split was in improving the model/reducing error.

For this model, the vast majority of importance comes from the top 3 variables: inspection_type, inspection_date, and census_tract. And of those, inspection_type is by far the most important variable with over twice as much importance as inspection_date and census_tract. Only 7 variables have a non-zero importance with longitude only barely having any relative importance. 

### 5. Application

a. This model could be used by the NYC health department to help allocate resources to prioritize health inspections for restaurants that are more likely to have a grade of Not A. It could also be used to predict the likelihood of a new restaurant receiving a grade of Not A or A. Further, the variable importance metric indicates that the inspection type is the most important predictive factor for the grade so this could provide a clue for additional study and investigation.

## Exercise 02

```{r}

# creating a subset of the diamonds data set
set.seed(20200301)

diamonds_sample <- diamonds %>%
  sample_n(5000) %>%
  select(price, carat, x, y, z)

```

### 1. Estimation

```{r}

set.seed(20200302)

# create a split object
diamonds_split <- initial_split(data = diamonds_sample, prop = 0.8)

# create the training and testing data
diamonds_train <- training(x = diamonds_split)
diamonds_test <- testing(x = diamonds_split)

# create recipe and normalize variables
predict_price_recipe <-
  recipe(formula = price ~ ., data = diamonds_train) %>%
  step_normalize(carat, x, y, z)

# create resamples
diamonds_folds <- vfold_cv(data = diamonds_train, v = 10)

# create model objects
predict_price_knn_mod <-
  nearest_neighbor(neighbors = tune()) %>%
  set_engine(engine = "kknn") %>%
  set_mode(mode = "regression")

predict_price_lreg_mod <- 
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

# create workflows
knn_workflow <-
  workflow() %>%
  add_model(spec = predict_price_knn_mod) %>%
  add_recipe(recipe = predict_price_recipe)

lreg_workflow <-
  workflow() %>%
  add_model(spec = predict_price_lreg_mod) %>%
  add_recipe(recipe = predict_price_recipe)

# create tuning grid for knn hyperparameters
knn_grid <- tibble(neighbors = c(3, 7, 11))

# estimate models
predict_price_knn_res <-
  knn_workflow %>%
  tune_grid(resamples = diamonds_folds,
            grid = knn_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))

predict_price_lreg_fit_rs <-
  lreg_workflow %>%
  fit_resamples(resamples = diamonds_folds)

```

### 2. Evaluation
```{r}
collect_metrics(predict_price_knn_res)
collect_metrics(predict_price_lreg_fit_rs)

lreg_metrics <- collect_metrics(predict_price_lreg_fit_rs, summarize = FALSE) %>%
  filter(.metric == "rmse") %>% 
  mutate(.config = "Lin Reg")

knn_metrics <- collect_metrics(predict_price_knn_res, summarize = FALSE) %>%
  filter(.metric == "rmse")

combined_metrics <- bind_rows(lreg_metrics, knn_metrics)

combined_metrics %>% 
  ggplot() +
  geom_line(aes(id, .estimate, group = .config, color = .config)) +
  geom_point(aes(id, .estimate, group = .config)) +
  scale_y_continuous() +
  scale_color_discrete(name = "Model", labels = c("Lin Reg", "KNN 3", "KNN 7", "KNN 11")) +
  labs(title = "Calculated RMSE Across the 10 Folds",
       y = "RMSE_hat",
       x = "Fold") +
  theme_minimal()

combined_metrics %>% 
  group_by(.config) %>% 
  summarize(mean(.estimate)) 

```
The average RMSE for each of the models is as follows:

Lin Reg:	1433.010			
KNN k = 3:	1585.314			
KNN k = 7:	1434.576			
KNN k = 11:	1394.683	

### 3. Prediction

```{r}

# create model object
predict_price_knn_best_mod <-
  nearest_neighbor(neighbors = 11) %>%
  set_engine(engine = "kknn") %>%
  set_mode(mode = "regression")

# create workflow
knn_best_workflow <-
  workflow() %>%
  add_model(spec = predict_price_knn_best_mod) %>%
  add_recipe(recipe = predict_price_recipe)

# fit model
predict_price_best_fit <- 
  knn_best_workflow %>% 
  fit(data = diamonds_train)

# make predictions with testing data
knn_best_predictions <-
  bind_cols(
    diamonds_test,
    predict(object = predict_price_best_fit, new_data = diamonds_test)
  )

# calculate the rmse for the testing data
rmse(data = knn_best_predictions, 
     truth = price, 
     estimate = .pred)

```
## Exercise 03

### 1. Set up
The data set is the same one used for Assignment 04, and includes information on economic development investments from across a variety of agencies and programs in the DC government for FY15-19. The data set was created by DMPED as required by the Economic Development Return on Investment Accountability Amendment Act of 2018. Fore more information see DMPED's [FY19 Economic Development Return on Investment Accountability Report](https://dmped.dc.gov/sites/default/files/dc/sites/dmped/publication/attachments/FY19%20ED%20Return%20on%20Investment%20Accountability%20Report.pdf).

The goal of this supervised machine learning application is to predict the total DC government investment in a given project based on project factors including affordable housing units, affordability levels, number of bedrooms, and others. This could be used by District government employees to estimate how much they should expect to contribute for future projects. It could also be used by advocates to flag potentially suspicious allocations of funds based on the expected level of government contribution. 

#### Data cleaning
```{r warning = FALSE, message = FALSE, results = FALSE}

# Making the variable names easier to work with 
dc_housing_investment <- read_csv("data/dc-housing-investment.csv") %>% 
  rename(
    year = `FY`,
    incentive_name = `Incentive Name`,
    recipient_name = `Recipient Name`,
    incentive_amount = `Incentive Amount`, 
    investment_address = `MAR Address of Investment`,
    ward = `MAR_Ward`,
    ami_30 = `30% AMI`,
    ami_50 = `50% AMI`,
    ami_60 = `60% AMI`,
    ami_80 = `80% AMI`,
    total_affordable = `Total Affordable Units Produced or Preserved`,
    one_br = `1 BRs`,
    two_br = `2 BRs`,
    three_br = `3 BRs`,
    four_plus_br = `4+ BRs`,
    studios = `Efficiency`,
    district_employed = `Number of District Residents Employed`,
    num_cbe = `Number of CBEs`,
    ) %>% 
  # Ensuring variables are consistent and converting to numeric
  # Note that the "HFA Revenue Bond Issuance" was renamed to "HFA Revenue Bond" in 2019 so this code makes all years consistent
  mutate(
    investment_address = str_to_upper(investment_address),
    ami_30 = as.numeric(ami_30),
    ami_50 = as.numeric(ami_50),
    ami_60 = as.numeric(ami_60),
    ami_80 = as.numeric(ami_80),
    total_affordable = as.numeric(total_affordable),
    one_br = as.numeric(one_br),
    two_br = as.numeric(two_br),
    three_br = as.numeric(three_br),
    four_plus_br = as.numeric(four_plus_br),
    studios = as.numeric(studios),
    district_employed = as.numeric(district_employed),
    num_cbe = as.numeric(num_cbe),
    incentive_name=replace(incentive_name, incentive_name=="HFA Revenue Bond Issuance", "HFA Revenue Bond"),
  )

# Some projects (denoted by address) had multiple investments from different programs so we want to know the total investment for each project
# There is probably a more efficient way to add this variable 
total_proj_data <- dc_housing_investment %>% 
  group_by(investment_address) %>% 
  summarize(total_proj_investment = sum(incentive_amount))

dc_housing_investment <- 
  left_join(dc_housing_investment, total_proj_data, by = "investment_address")

# Also probably a more efficient way to do this, but this removes any economic development incentives that did not produce any affordable housing units across the data set and consolidates investments that are at the same address 
min_affordable_units <- dc_housing_investment %>% 
  group_by(incentive_name) %>% 
  summarize(incentive_total_affordable = sum(total_affordable, na.rm = TRUE)) %>% 
  filter(incentive_total_affordable > 0) %>% 
  pull(incentive_name)

dc_housing_investment <- dc_housing_investment %>% 
  filter(incentive_name %in% min_affordable_units) %>% 
  distinct(investment_address, .keep_all= TRUE) %>% 
  filter(!is.na(total_affordable))

```

#### Splitting data set
```{r}

set.seed(20201109)
# create a split object
housing_investment_split <- initial_split(data = dc_housing_investment, prop = 0.75)
# create the training and testing data
housing_investment_train <- training(x = housing_investment_split)
housing_investment_test <- testing(x = housing_investment_split)

```

#### Exploratory data analysis

```{r}

housing_investment_train %>%
  ggplot(aes(x = total_affordable, y = total_proj_investment)) +
  geom_point() +
  labs(title = "Total Investment by Affordable Units") +
  scale_y_continuous(labels = comma) +
  theme_minimal()

housing_investment_train %>%
  ggplot(aes(x = three_br, y = total_proj_investment)) +
  geom_point() +
  labs(title = "Total Investment by Three Bedroom Units") +
  scale_y_continuous(labels = comma) +
  theme_minimal()

housing_investment_train %>%
  ggplot(aes(x = ami_30, y = total_proj_investment)) +
  geom_point() +
  labs(title = "Total Investment by Units at 30% AMI") +
  scale_y_continuous(labels = comma) +
  theme_minimal()

housing_investment_train %>% 
  ggplot(mapping = aes(
    x = ward, 
    y = total_proj_investment,
    )) +
  geom_dotplot( 
    binaxis='y', 
    stackdir='center', 
    binwidth = 2000000) +
  scale_y_continuous(labels = comma) +
  stat_summary(fun.y=median, geom="point", shape=18,
                 size=3, color="red") +
  labs(
    x = "Ward",
    y = "Total Project Investment",
    title = "Total Project Investment by Ward"
  )

housing_investment_train %>% 
  ggplot(mapping = aes(
    x = incentive_name, 
    y = total_proj_investment,
    )) +
  geom_dotplot( 
    binaxis='y', 
    stackdir='center', 
    binwidth = 1000000) +
  scale_y_continuous(labels = comma) +
  stat_summary(fun.y=median, geom="point", shape=18,
                 size=3, color="red") +
  labs(
    x = "Incentive Name",
    y = "Total Project Investment",
    title = "Total Project Investment by Incentive"
  ) +
  coord_flip()

```
#### Error metric
```{r}

housing_investment_train %>% summarize(sd(total_proj_investment),
                                       quantile(total_proj_investment))

housing_investment_train %>% 
  ggplot() + 
  geom_histogram(aes(x = total_proj_investment), 
                 binwidth = 1000000) + 
  scale_x_continuous(labels = comma)

```
Since this is a regression application with a continuous target variable, I will be using root mean square error (RMSE) as my error metric. As compared to mean absolute error RMSE will give somewhat more weight to outliers. 

The standard deviation of the total project investment is quite high while the distribution is right skewed. The costs of an error depend on the context in which the model is being used. If the model is the only thing being used to project costs and investment for affordable housing then errors could be quite costly since there are limited funds for affordable housing and this is a crucial expenditure for the well-being of many DC residents. If the model is simply being used as one tool for analysis and more as a check on the reasonableness of investment then errors will be less costly. Regardless, given that most projects fall below \$5,000,000 in total investment, an error of greater than \$5,000,000 would indicate the model isn't very useful. 

### 2. Coming up with models

Predictor variables were chosen based on their potential to impact the cost of a project or the value to the DC government.

**Model 1**

For KNN all predictor variables must be continuous.

Predictor variables: 

- Total Affordable Units
- 30% AMI Units
- 50% AMI Units
- 60% AMI Units
- 80% AMI Units
- One Bedroom Units
- Two Bedroom Units
- Three Bedroom Units
- Four+ Bedroom Units
- Studio Units

Preprocessing: 
Algorithm: K-Nearest Neighbors
Hyperparameters:

**Model 2**

Predictor variables: 

- Incentive Name
- Ward
- Total Affordable Units
- 30% AMI Units
- 50% AMI Units
- 60% AMI Units
- 80% AMI Units
- One Bedroom Units
- Two Bedroom Units
- Three Bedroom Units
- Four+ Bedroom Units
- Studio Units
- DC residents employed
- Number of certified small businesses served

Preprocessing:
Algorithm:
Hyperparameters:

**Model 3**

Predictor variables: 

- Incentive Name
- Ward
- Total Affordable Units
- 30% AMI Units
- 50% AMI Units
- 60% AMI Units
- 80% AMI Units
- One Bedroom Units
- Two Bedroom Units
- Three Bedroom Units
- Four+ Bedroom Units
- Studio Units
- DC residents employed
- Number of certified small businesses served

Preprocessing:
Algorithm:
Hyperparameters:

• State the predictor variables included in the model and necessary preprocessing for each variable
• Use at least least two different types of preprocessing
• Use at least two different algorithms (i.e. linear regression, KNN, CART, random forest).
• Use at least one algorithm with hyperparameters. Your first specification should be an algorithm
with a hyperparameter(s).





