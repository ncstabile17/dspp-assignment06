---
title: "Assignment 07"
author: "Nick Stabile"
date: "11/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(tidymodels)
library(vip)
```

## Exercise 01

### Creating the data set

```{r message = FALSE, warning = FALSE}

# use this url to download the data directly into R
df <- read_csv("https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv")

# clean names with janitor
sampled_df <- df %>% 
  janitor::clean_names() 

# create an inspection year variable
sampled_df <- sampled_df %>%
  mutate(inspection_date = mdy(inspection_date)) %>%
  mutate(inspection_year = year(inspection_date))

# get most-recent inspection
sampled_df <- sampled_df %>%
  group_by(camis) %>%
  filter(inspection_date == max(inspection_date)) %>%
  ungroup()

# subset the data
sampled_df <- sampled_df %>%
  select(camis, boro, zipcode, cuisine_description, inspection_date,
         action, violation_code, violation_description, grade,
         inspection_type, latitude, longitude, council_district,
         census_tract, inspection_year, critical_flag) %>%
  filter(complete.cases(.)) %>%
  filter(inspection_year >= 2017) %>%
  filter(grade %in% c("A", "B", "C")) 

# create the binary target variable
sampled_df <- sampled_df %>%
  mutate(grade = if_else(grade == "A", "A", "Not A")) %>%
  mutate(grade = as.factor(grade))

# create extra predictors
sampled_df <- sampled_df %>%
  group_by(boro, zipcode, cuisine_description, inspection_date,
           action, violation_code, violation_description, grade,
           inspection_type, latitude, longitude, council_district,
           census_tract, inspection_year)  %>%
  mutate(vermin = str_detect(violation_description, pattern = "mice|rats|vermin|roaches")) %>%
  summarize(violations = n(),
            vermin_types = sum(vermin),
            critical_flags = sum(critical_flag == "Y")) %>%
  ungroup()

# write the data
write_csv(sampled_df, "restaurant_grades.csv")


```

### 1. Estimate a model

```{r message = FALSE, warning = FALSE}
set.seed(20201020)

# create a split object
restaurant_grades_split <- initial_split(data = sampled_df, prop = 0.8)

# create the training and testing data
restaurant_grades_train <- training(x = restaurant_grades_split)
restaurant_grades_test <- testing(x = restaurant_grades_split)

# create recipe and downsample grades
predict_grade_cart_recipe <-
  recipe(formula = grade ~ ., data = restaurant_grades_train) %>%
  themis::step_downsample(grade)

# create model object
predict_grade_cart_mod <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

# create a workflow with the recipe
predict_grade_cart_workflow <-
  workflow() %>%
  add_model(spec = predict_grade_cart_mod) %>%
  add_recipe(recipe = predict_grade_cart_recipe)

# fit the model using the workflow
predict_grade_cart_fit <- 
  predict_grade_cart_workflow %>% 
  fit(data = restaurant_grades_train)

```

### 2. Evaluate the model

```{r}

# use the model to get the predictions on the test data
grade_predictions <- bind_cols(
  restaurant_grades_test,
  predict(object = predict_grade_cart_fit, new_data = restaurant_grades_test)
)

# create a confusion matrix based on predictions of test data
grade_pred_conf_matrix <- table(grade_predictions$.pred_class, grade_predictions$grade)

grade_pred_conf_matrix

# Calculate precision and recall/sensitivity "by hand"
grade_pred_precision <- grade_pred_conf_matrix[1]/(grade_pred_conf_matrix[1] + grade_pred_conf_matrix[3])
grade_pred_precision

grade_pred_recall <- grade_pred_conf_matrix[1]/(grade_pred_conf_matrix[1] + grade_pred_conf_matrix[2])
grade_pred_recall

# Calculate precision and recall/sensitivity with tidymodels
precision(data = grade_predictions, truth = grade, estimate = .pred_class)

recall(data = grade_predictions, truth = grade, estimate = .pred_class)

```
c. The precision and recall calculated "by hand" match the values for precision and recall calculated using the tidymodels functions.

d. The model is very precise, with over 99% precision, meaning that it's almost almost correct when it predicts a restaurant will have an A grade. The model does somewhat worse on recall, with around 81% sensitivity, meaning that when the grade should be an A, there is about a 20% chance the model will predict Not A. In this sense it is a somewhat conservative model: it does not predict an A grade when it shouldn't (false positive), but it occasionally fails to predict A when it should have (false negative). This is probably a good thing for restaurant patrons since it is overly protective of their health, but would harm the marginal restaurant owner who should be receiving an A grade, but is not.

### 3. Improvement

The model is quite precise, but could at least improve its recall (among other metrics) so we would want to make improvements that would ideally reduce false negatives by providing more information to correctly classify restaurants that should've been given an A grade but were not. More information about a restaurant owner's history could be potentially useful: grades from previous years or grades from additional or previous restaurants may be helpful in predicting the current restaurant grade. Additionally, having more specific and descriptive location information (i.e. combining lat/lon to determine relative distance) could help in predicting pockets of restaurants that may receive failing grades because of environmental conditions like the presence of vermin. Experimenting with different algorithms could also be a source of improvement including using a random forest algorithm, which is typically more accurate and still slightly interpretable.

### 4. Variable Importance

```{r}
predict_grade_cart_fit %>% 
  pull_workflow_fit() %>%
  vip(num_features = 10)
```

In CART decision tree models, for each split in the tree the model picks the most efficient predictor or variable to split the data, or the one that reduces misclassification the most/improves the model the most as compared to all other options. Calculating variable importance then requires comparing how often variables are used to perform that split and how relatively valuable that split was in improving the model/reducing error.

For this model, the vast majority of importance comes from the top 3 variables: inspection_type, inspection_date, and census_tract. And of those, inspection_type is by far the most important variable with over twice as much importance as inspection_date and census_tract. Only 7 variables have a non-zero importance with longitude only barely having any relative importance. 

### 5. Application

a. This model could be used by the NYC health department to help allocate resources to prioritize health inspections for restaurants that are more likely to have a grade of Not A. It could also be used to predict the likelihood of a new restaurant receiving a grade of Not A or A. Further, the variable importance metric indicates that the inspection type is the most important predictive factor for the grade so this could provide a clue for additional study and investigation.

## Exercise 02

```{r}

# creating a subset of the diamonds data set
set.seed(20200301)

diamonds_sample <- diamonds %>%
  sample_n(5000) %>%
  select(price, carat, x, y, z)

```

### 1. Estimation

```{r}

set.seed(20200302)

# create a split object
diamonds_split <- initial_split(data = diamonds_sample, prop = 0.8)

# create the training and testing data
diamonds_train <- training(x = diamonds_split)
diamonds_test <- testing(x = diamonds_split)

# create recipe and normalize variables
predict_price_recipe <-
  recipe(formula = price ~ ., data = diamonds_train) %>%
  step_normalize(carat, x, y, z)

# create resamples
diamonds_folds <- vfold_cv(data = diamonds_train, v = 10)

# create model objects
predict_price_knn_mod <-
  nearest_neighbor(neighbors = tune()) %>%
  set_engine(engine = "kknn") %>%
  set_mode(mode = "regression")

predict_price_lreg_mod <- 
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

# create workflows
knn_workflow <-
  workflow() %>%
  add_model(spec = predict_price_knn_mod) %>%
  add_recipe(recipe = predict_price_recipe)

lreg_workflow <-
  workflow() %>%
  add_model(spec = predict_price_lreg_mod) %>%
  add_recipe(recipe = predict_price_recipe)

# create tuning grid for knn hyperparameters
knn_grid <- tibble(neighbors = c(3, 7, 11))

# estimate models
predict_price_knn_res <-
  knn_workflow %>%
  tune_grid(resamples = diamonds_folds,
            grid = knn_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))

predict_price_lreg_fit_rs <-
  lreg_workflow %>%
  fit_resamples(resamples = diamonds_folds)

```

### 2. Evaluation
```{r}
collect_metrics(predict_price_knn_res)
collect_metrics(predict_price_lreg_fit_rs)

lreg_metrics <- collect_metrics(predict_price_lreg_fit_rs, summarize = FALSE) %>%
  filter(.metric == "rmse") %>% 
  mutate(.config = "Lin Reg")

knn_metrics <- collect_metrics(predict_price_knn_res, summarize = FALSE) %>%
  filter(.metric == "rmse")

combined_metrics <- bind_rows(lreg_metrics, knn_metrics)

combined_metrics %>% 
  ggplot() +
  geom_line(aes(id, .estimate, group = .config, color = .config)) +
  geom_point(aes(id, .estimate, group = .config)) +
  scale_y_continuous() +
  scale_color_discrete(name = "Model", labels = c("Lin Reg", "KNN 3", "KNN 7", "KNN 11")) +
  labs(title = "Calculated RMSE Across the 10 Folds",
       y = "RMSE_hat",
       x = "Fold") +
  theme_minimal()

combined_metrics %>% 
  group_by(.config) %>% 
  summarize(mean(.estimate)) 

```
The average RMSE for each of the models is as follows:

Lin Reg:	1433.010			
KNN k = 3:	1585.314			
KNN k = 7:	1434.576			
KNN k = 11:	1394.683	

### 3. Prediction

```{r}

# create model object
predict_price_knn_best_mod <-
  nearest_neighbor(neighbors = 11) %>%
  set_engine(engine = "kknn") %>%
  set_mode(mode = "regression")

# create workflow
knn_best_workflow <-
  workflow() %>%
  add_model(spec = predict_price_knn_best_mod) %>%
  add_recipe(recipe = predict_price_recipe)

# fit model
predict_price_best_fit <- 
  knn_best_workflow %>% 
  fit(data = diamonds_train)

# make predictions with testing data
knn_best_predictions <-
  bind_cols(
    diamonds_test,
    predict(object = predict_price_best_fit, new_data = diamonds_test)
  )

# calculate the rmse for the testing data
rmse(data = knn_best_predictions, 
     truth = price, 
     estimate = .pred)

```
## Exercise 03: Supervised ML Option

### 1. Set up
The data set is the same one used for Assignment 04, and includes information on economic development investments from across a variety of agencies and programs in the DC government for FY15-19. The data set was created by DMPED as required by the Economic Development Return on Investment Accountability Amendment Act of 2018. Fore more information see DMPED's [FY19 Economic Development Return on Investment Accountability Report](https://dmped.dc.gov/sites/default/files/dc/sites/dmped/publication/attachments/FY19%20ED%20Return%20on%20Investment%20Accountability%20Report.pdf).

The goal of this supervised machine learning application is to predict the total DC government investment in a given project based on project factors including affordable housing units, affordability levels, number of bedrooms, and others. This could be used by District government employees to estimate how much they should expect to contribute for future projects. It could also be used by advocates to flag potentially suspicious allocations of funds based on the expected level of government contribution. 

#### Data cleaning
```{r warning = FALSE, message = FALSE, results = FALSE}

# Making the variable names easier to work with 
dc_housing_investment <- read_csv("data/dc-housing-investment.csv") %>% 
  rename(
    year = `FY`,
    incentive_name = `Incentive Name`,
    recipient_name = `Recipient Name`,
    incentive_amount = `Incentive Amount`, 
    investment_address = `MAR Address of Investment`,
    ward = `MAR_Ward`,
    ami_30 = `30% AMI`,
    ami_50 = `50% AMI`,
    ami_60 = `60% AMI`,
    ami_80 = `80% AMI`,
    total_affordable = `Total Affordable Units Produced or Preserved`,
    one_br = `1 BRs`,
    two_br = `2 BRs`,
    three_br = `3 BRs`,
    four_plus_br = `4+ BRs`,
    studios = `Efficiency`,
    district_employed = `Number of District Residents Employed`,
    num_cbe = `Number of CBEs`,
    ) %>% 
  # Ensuring variables are consistent and converting to numeric
  # Note that the "HFA Revenue Bond Issuance" was renamed to "HFA Revenue Bond" in 2019 so this code makes all years consistent
  mutate(
    investment_address = str_to_upper(investment_address),
    ami_30 = as.numeric(ami_30),
    ami_50 = as.numeric(ami_50),
    ami_60 = as.numeric(ami_60),
    ami_80 = as.numeric(ami_80),
    total_affordable = as.numeric(total_affordable),
    one_br = as.numeric(one_br),
    two_br = as.numeric(two_br),
    three_br = as.numeric(three_br),
    four_plus_br = as.numeric(four_plus_br),
    studios = as.numeric(studios),
    district_employed = as.numeric(district_employed),
    num_cbe = as.numeric(num_cbe),
    incentive_name=replace(incentive_name, incentive_name=="HFA Revenue Bond Issuance", "HFA Revenue Bond"),
  )

# Some projects (denoted by address) had multiple investments from different programs so we want to know the total investment for each project
# There is probably a more efficient way to add this variable 
total_proj_data <- dc_housing_investment %>% 
  group_by(investment_address) %>% 
  summarize(total_proj_investment = sum(incentive_amount))

dc_housing_investment <- 
  left_join(dc_housing_investment, total_proj_data, by = "investment_address")

# Also probably a more efficient way to do this, but this removes any economic development incentives that did not produce any affordable housing units across the data set and consolidates investments that are at the same address 
min_affordable_units <- dc_housing_investment %>% 
  group_by(incentive_name) %>% 
  summarize(incentive_total_affordable = sum(total_affordable, na.rm = TRUE)) %>% 
  filter(incentive_total_affordable > 0) %>% 
  pull(incentive_name)

dc_housing_investment <- dc_housing_investment %>% 
  filter(incentive_name %in% min_affordable_units) %>% 
  distinct(investment_address, .keep_all= TRUE) %>% 
  filter(!is.na(total_affordable)) %>% 
  replace_na(list(total_affordable = 0, ami_30 = 0, ami_50 = 0, ami_60 = 0, ami_80 = 0, one_br = 0, two_br = 0, three_br = 0, four_plus_br = 0, studios = 0))

```

#### Splitting data set
```{r}

set.seed(20201109)
# create a split object
housing_investment_split <- initial_split(data = dc_housing_investment, prop = 0.75)
# create the training and testing data
housing_investment_train <- training(x = housing_investment_split)
housing_investment_test <- testing(x = housing_investment_split)

```

#### Exploratory data analysis

```{r}

housing_investment_train %>%
  ggplot(aes(x = total_affordable, y = total_proj_investment)) +
  geom_point() +
  labs(title = "Total Investment by Affordable Units") +
  scale_y_continuous(labels = comma) +
  theme_minimal()

housing_investment_train %>%
  ggplot(aes(x = three_br, y = total_proj_investment)) +
  geom_point() +
  labs(title = "Total Investment by Three Bedroom Units") +
  scale_y_continuous(labels = comma) +
  theme_minimal()

housing_investment_train %>%
  ggplot(aes(x = ami_30, y = total_proj_investment)) +
  geom_point() +
  labs(title = "Total Investment by Units at 30% AMI") +
  scale_y_continuous(labels = comma) +
  theme_minimal()

housing_investment_train %>% 
  ggplot(mapping = aes(
    x = ward, 
    y = total_proj_investment,
    )) +
  geom_dotplot( 
    binaxis='y', 
    stackdir='center', 
    binwidth = 2000000) +
  scale_y_continuous(labels = comma) +
  stat_summary(fun=median, geom="point", shape=18,
                 size=3, color="red") +
  labs(
    x = "Ward",
    y = "Total Project Investment",
    title = "Total Project Investment by Ward"
  )

housing_investment_train %>% 
  ggplot(mapping = aes(
    x = incentive_name, 
    y = total_proj_investment,
    )) +
  geom_dotplot( 
    binaxis='y', 
    stackdir='center', 
    binwidth = 1000000) +
  scale_y_continuous(labels = comma) +
  stat_summary(fun=median, geom="point", shape=18,
                 size=3, color="red") +
  labs(
    x = "Incentive Name",
    y = "Total Project Investment",
    title = "Total Project Investment by Incentive"
  ) +
  coord_flip()

```

#### Error metric
```{r}

housing_investment_train %>% summarize(sd(total_proj_investment),
                                       quantile(total_proj_investment))

housing_investment_train %>% 
  ggplot() + 
  geom_histogram(aes(x = total_proj_investment), 
                 binwidth = 1000000) + 
  scale_x_continuous(labels = comma)

```

Since this is a regression application with a continuous target variable, I will be using root mean square error (RMSE) as my error metric. As compared to mean absolute error RMSE will give somewhat more weight to outliers. 

The standard deviation of the total project investment is quite high while the distribution is right skewed. The costs of an error depend on the context in which the model is being used. If the model is the only thing being used to project costs and investment for affordable housing then errors could be quite costly since there are limited funds for affordable housing and this is a crucial expenditure for the well-being of many DC residents. If the model is simply being used as one tool for analysis and more as a check on the reasonableness of investment then errors will be less costly. Regardless, given that most projects fall below \$5,000,000 in total investment, an error of greater than \$5,000,000 would indicate the model is not very accurate for most observations, but this would still be a reasonably good error in relation to the total range of the observations. 

### 2. Coming up with models

Predictor variables were chosen based on their potential to impact the cost of a project or the value to the DC government.

**Model 1**

This model specification will use K-Nearest Neighbors as the algorithm. For KNN all predictor variables must be continuous and all must be normalized as one of the preprocessing steps. The *K* hyperparameter will be set using a tuning grid for a range of potential values.

Predictor variables: 

- Total Affordable Units
- 30% AMI Units
- 50% AMI Units
- 60% AMI Units
- 80% AMI Units
- One Bedroom Units
- Two Bedroom Units
- Three Bedroom Units
- Four+ Bedroom Units
- Studio Units

**Model 2**

This model specification will use a CART algorithm. All variables will have missing values imputed as part of the preprocessing.

Predictor variables: 

- Incentive Name
- Ward
- Total Affordable Units
- 30% AMI Units
- 50% AMI Units
- 60% AMI Units
- 80% AMI Units
- One Bedroom Units
- Two Bedroom Units
- Three Bedroom Units
- Four+ Bedroom Units
- Studio Units
- DC residents employed
- Number of certified small businesses served

**Model 3**

This model specification will use a Random Forest algorithm. So that observations are not ignored, all variables will have missing values imputed as part of the preprocessing. The hyperparameter for number of trees will be tuned to determine the optimal number of trees.

Predictor variables: 

- Incentive Name
- Ward
- Total Affordable Units
- 30% AMI Units
- 50% AMI Units
- 60% AMI Units
- 80% AMI Units
- One Bedroom Units
- Two Bedroom Units
- Three Bedroom Units
- Four+ Bedroom Units
- Studio Units
- DC residents employed
- Number of certified small businesses served

### 3. Estimation

```{r}

# create recipe and normalize variables
predict_investment_recipe <-
  recipe(formula = total_proj_investment ~ total_affordable + ami_30 + ami_50 + ami_60 + ami_80 + one_br + two_br + three_br + four_plus_br + studios,
         data = housing_investment_train) %>%
  step_normalize(total_affordable, ami_30, ami_50, ami_60, ami_80, one_br, two_br, three_br, four_plus_br, studios)

# create resamples
housing_investment_folds <- vfold_cv(data = housing_investment_train, v = 10)

# create model object
predict_investment_knn_mod <-
  nearest_neighbor(neighbors = tune()) %>%
  set_engine(engine = "kknn") %>%
  set_mode(mode = "regression")

# create workflow
investment_knn_workflow <-
  workflow() %>%
  add_model(spec = predict_investment_knn_mod) %>%
  add_recipe(recipe = predict_investment_recipe)

# create tuning grid for knn hyperparameters
investment_knn_grid <- tibble(neighbors = c(1, 3, 7, 9, 11, 13))

# estimate models
predict_investment_knn_res <-
  investment_knn_workflow %>%
  tune_grid(resamples = housing_investment_folds,
            grid = investment_knn_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))

collect_metrics(predict_investment_knn_res)
```
### 4. Interpretation
a. The average RMSE across the training resamples ranged from just under 15,000,000 to just over 19,000,000 with the average RMSE decreasing with increasing values of K. This means that when predicting total project investment, the best model for the first specification was off by about $15M on average. 

b. Based purely on the general accuracy of the two algorithms, the second specification will likely be worse than the first specification because typically KNN is more accurate than decision trees. The second specification will be able to incorporate categorical variables including the incentive name and the Ward that the project is located in, which were not included in the first specification. Intuitively, you might expect these variables to play an important role in project investment, but based on the exploratory data analysis, there does not seem to be a strong relationship so I would not expect these additional variables to make that much of a difference. 

c. Again, based on the algorithm, I would expect the third specification to outperform the first specification because random forest models are typically more accurate than KNN. Similar to the second specification, the third specification may also benefit slightly from the additional variables that were not included in the first specification because they were categorical and I wanted to slightly cut down on dimensionality for the KNN specification. Another advantage of the random forest specification is that it might be interesting and useful to create a variable importance plot, which would help to explain the features that play the biggest role in predicting total project investment. 

d. I don't feel like I have a great handle on the ways that different preprocessing and feature engineering are best utilized in the modeling process. To improve the model, I would spend more time digging into the data set and understanding the ways that I could use feature engineering techniques to improve the model's accuracy. I would also add additional data from other sources that could play an important role in project investment. For example, I would want to add information on the number of market-rate housing units in each project since market-rate units can be used to cross-subsidize affordable units and therefore might lead to less of a need for government investment. More information on localized land values would also be useful as it's likely that projects located in areas with higher land values would require more government investment. Although this might be captured to some degree by project Ward location, more specific land value information could be useful.

e. For the first specification, the optimal K was 13 with an average RMSE of 14,891,041. As previously mentioned, most of the observations had a true value for total project investment of less than 5,000,000 so this indicates that on average the model would not be very accurate for projects with lower levels of investment. This RMSE value, however, is lower than the standard deviation for the training data and is relatively good in comparison to the total range of values for project investment. In concrete terms, though, being off by almost $15M is not very useful in the context of how this model would be used, unless you're talking about the most expensive projects. 


