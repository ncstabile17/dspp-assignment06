---
title: "Assignment 07"
author: "Nick Stabile"
date: "11/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(tidymodels)
library(vip)
```

## Exercise 01

### Creating the data set

```{r}

# use this url to download the data directly into R
df <- read_csv("https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv")

# clean names with janitor
sampled_df <- df %>% 
  janitor::clean_names() 

# create an inspection year variable
sampled_df <- sampled_df %>%
  mutate(inspection_date = mdy(inspection_date)) %>%
  mutate(inspection_year = year(inspection_date))

# get most-recent inspection
sampled_df <- sampled_df %>%
  group_by(camis) %>%
  filter(inspection_date == max(inspection_date)) %>%
  ungroup()

# subset the data
sampled_df <- sampled_df %>%
  select(camis, boro, zipcode, cuisine_description, inspection_date,
         action, violation_code, violation_description, grade,
         inspection_type, latitude, longitude, council_district,
         census_tract, inspection_year, critical_flag) %>%
  filter(complete.cases(.)) %>%
  filter(inspection_year >= 2017) %>%
  filter(grade %in% c("A", "B", "C")) 

# create the binary target variable
sampled_df <- sampled_df %>%
  mutate(grade = if_else(grade == "A", "A", "Not A")) %>%
  mutate(grade = as.factor(grade))

# create extra predictors
sampled_df <- sampled_df %>%
  group_by(boro, zipcode, cuisine_description, inspection_date,
           action, violation_code, violation_description, grade,
           inspection_type, latitude, longitude, council_district,
           census_tract, inspection_year)  %>%
  mutate(vermin = str_detect(violation_description, pattern = "mice|rats|vermin|roaches")) %>%
  summarize(violations = n(),
            vermin_types = sum(vermin),
            critical_flags = sum(critical_flag == "Y")) %>%
  ungroup()

# write the data
write_csv(sampled_df, "restaurant_grades.csv")


```

### 1. Estimate a model

```{r}
set.seed(20201020)

# create a split object
restaurant_grades_split <- initial_split(data = sampled_df, prop = 0.8)

# create the training and testing data
restaurant_grades_train <- training(x = restaurant_grades_split)
restaurant_grades_test <- testing(x = restaurant_grades_split)

# create recipe and downsample grades
predict_grade_cart_recipe <-
  recipe(formula = grade ~ ., data = restaurant_grades_train) %>%
  themis::step_downsample(grade)

# create model object
predict_grade_cart_mod <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

# create a workflow with the recipe
predict_grade_cart_workflow <-
  workflow() %>%
  add_model(spec = predict_grade_cart_mod) %>%
  add_recipe(recipe = predict_grade_cart_recipe)

# fit the model using the workflow
predict_grade_cart_fit <- 
  predict_grade_cart_workflow %>% 
  fit(data = restaurant_grades_train)

```

### 2. Evaluate the model

```{r}

# use the model to get the predictions on the test data
grade_predictions <- bind_cols(
  restaurant_grades_test,
  predict(object = predict_grade_cart_fit, new_data = restaurant_grades_test)
)

# create a confusion matrix based on predictions of test data
grade_pred_conf_matrix <- table(grade_predictions$.pred_class, grade_predictions$grade)

grade_pred_conf_matrix

# Calculate precision and recall/sensitivity "by hand"
grade_pred_precision <- grade_pred_conf_matrix[1]/(grade_pred_conf_matrix[1] + grade_pred_conf_matrix[3])
grade_pred_precision

grade_pred_recall <- grade_pred_conf_matrix[1]/(grade_pred_conf_matrix[1] + grade_pred_conf_matrix[2])
grade_pred_recall

# Calculate precision and recall/sensitivity with tidymodels
precision(data = grade_predictions, truth = grade, estimate = .pred_class)

recall(data = grade_predictions, truth = grade, estimate = .pred_class)

```
c. The precision and recall calculated "by hand" match the values for precision and recall calculated using the tidymodels functions.

d. The model is very precise, with over 99% precision, meaning that it's almost almost correct when it predicts a restaurant will have an A grade. The model does somewhat worse on recall, with around 81% sensitivity, meaning that when the grade should be an A, there is about a 20% chance the model will predict Not A. In this sense it is a somewhat conservative model: it does not predict an A grade when it shouldn't (false positive), but it occasionally fails to predict A when it should have (false negative). This is probably a good thing for restaurant patrons since it is overly protective of their health, but would harm the marginal restaurant owner who should be receiving an A grade, but is not.

### 3. Improvement

The model is quite precise, but could at least improve its recall (among other metrics) so we would want to make improvements that would ideally reduce false negatives by providing more information to correctly classify restaurants that should've been given an A grade but were not. More information about a restaurant owner's history could be potentially useful: grades from previous years or grades from additional or previous restaurants may be helpful in predicting the current restaurant grade. Additionally, having more specific and descriptive location information (i.e. combining lat/lon to determine relative distance) could help in predicting pockets of restaurants that may receive failing grades because of environmental conditions like the presence of vermin. Experimenting with different algorithms could also be a source of improvement including using a random forest algorithm, which is typically more accurate and still slightly interpretable.

### 4. Variable Importance

```{r}
predict_grade_cart_fit %>% 
  pull_workflow_fit() %>%
  vip(num_features = 10)
```

In CART decision tree models, for each split in the tree the model picks the most efficient predictor or variable to split the data, or the one that reduces misclassification the most/improves the model the most as compared to all other options. Calculating variable importance then requires comparing how often variables are used to perform that split and how relatively valuable that split was in improving the model/reducing error.

For this model, the vast majority of importance comes from the top 3 variables: inspection_type, inspection_date, and census_tract. And of those, inspection_type is by far the most important variable with over twice as much importance as inspection_date and census_tract. Only 7 variables have a non-zero importance with longitude only barely having any relative importance. 

### 5. Application

a. How might this model be used by an ombudsman working for the NYC health department?
This model could be used by the NYC health department to help allocate resources to prioritize health inspections for restaurants that are more likely to have a grade of Not A. It could also be used to predict the likelihood of a new restaurant receiving a grade of Not A or A. Further, the variable importance metric indicates that the inspection type is the most important predictive factor for the grade so this could provide a clue for additional study and investigation.

## Exercise 02

```{r}

# creating a subset of the diamonds data set
set.seed(20200301)

diamonds_sample <- diamonds %>%
  sample_n(5000) %>%
  select(price, carat, x, y, z)

```

### 1. Estimation

```{r}

set.seed(20200302)

# create a split object
diamonds_split <- initial_split(data = diamonds_sample, prop = 0.8)

# create the training and testing data
diamonds_train <- training(x = diamonds_split)
diamonds_test <- testing(x = diamonds_split)

# create recipe and normalize variables
predict_price_recipe <-
  recipe(formula = price ~ ., data = diamonds_train) %>%
  step_normalize(carat, x, y, z)

# create resamples
diamonds_folds <- vfold_cv(data = diamonds_train, v = 10)

# create model objects
predict_price_knn_mod <-
  nearest_neighbor(neighbors = tune()) %>%
  set_engine(engine = "kknn") %>%
  set_mode(mode = "regression")

predict_price_lreg_mod <- 
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

# create workflows
knn_workflow <-
  workflow() %>%
  add_model(spec = predict_price_knn_mod) %>%
  add_recipe(recipe = predict_price_recipe)

lreg_workflow <-
  workflow() %>%
  add_model(spec = predict_price_lreg_mod) %>%
  add_recipe(recipe = predict_price_recipe)

# create tuning grid for knn hyperparameters
knn_grid <- tibble(neighbors = c(3, 7, 11))

# estimate models
predict_price_knn_res <-
  knn_workflow %>%
  tune_grid(resamples = diamonds_folds,
            grid = knn_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))

predict_price_lreg_fit_rs <-
  lreg_workflow %>%
  fit_resamples(resamples = diamonds_folds)

```

### 2. Evaluation
```{r}
collect_metrics(predict_price_knn_res)
collect_metrics(predict_price_lreg_fit_rs)

lreg_metrics <- collect_metrics(predict_price_lreg_fit_rs, summarize = FALSE) %>%
  filter(.metric == "rmse") %>% 
  mutate(.config = "Lin Reg")

knn_metrics <- collect_metrics(predict_price_knn_res, summarize = FALSE) %>%
  filter(.metric == "rmse")

combined_metrics <- bind_rows(lreg_metrics, knn_metrics)

combined_metrics %>% 
  ggplot() +
  geom_line(aes(id, .estimate, group = .config, color = .config)) +
  geom_point(aes(id, .estimate, group = .config)) +
  scale_y_continuous() +
  scale_color_discrete(name = "Model", labels = c("Lin Reg", "KNN 3", "KNN 7", "KNN 11")) +
  labs(title = "Calculated RMSE Across the 10 Folds",
       y = "RMSE_hat",
       x = "Fold") +
  theme_minimal()

combined_metrics %>% 
  group_by(.config) %>% 
  summarize(mean(.estimate)) 

```
The average RMSE for each of the models is as follows:
Lin Reg:	1433.010			
KNN k = 3:	1585.314			
KNN k = 7:	1434.576			
KNN k = 11:	1394.683	

### 3. Prediction

