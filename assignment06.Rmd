---
title: "Assignment 06"
author: "Nick Stabile"
date: "10/26/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidymodels)
library(tidyverse)
library(parsnip)
```

## Exercise 01

$$\begin{eqnarray}
MSE & = & \frac{1}{5}\sum_{i=1}^5(y_i - \hat{y_i})^2 \\
& = & \frac{1}{5}((1-2)^2 + (2-2)^2 + (3-1)^2 + (4-8)^2 + (5-4)^2) \\
& = & 4.4
\end{eqnarray}$$

$$\begin{eqnarray}
RMSE & = & \sqrt{\frac{1}{5}\sum_{i=1}^5(y_i - \hat{y_i})^2} \\
& = & \sqrt{\frac{1}{5}((1-2)^2 + (2-2)^2 + (3-1)^2 + (4-8)^2 + (5-4)^2)} \\
& = & 2.10
\end{eqnarray}$$

$$\begin{eqnarray}
MAE & = & \frac{1}{5}\sum_{i=1}^5|y_i - \hat{y_i}| \\
& = & \frac{1}{5}(|1-2| + |2-2| + |3-1| + |4-8| + |5-4|) \\
& = & 1.6
\end{eqnarray}$$

As compared to MAE, RMSE puts more emphasis on outliers (i.e. predictions with errors of greater magnitude). This is due to the fact that RMSE first squares the difference between the actual and predicted values so gives greater weight to outliers, whereas MAE takes the absolute value of the differences rather than squaring. 

## Exercise 02

### 1. Confusion matrix
```{r, out.width = "50%"}
include_graphics("confusion-matrix-assignment06e02.JPG")
```

### 2. Accuracy
$$\begin{eqnarray}
Accuracy & = & \frac{True Positive + True Negative}{Total} \\
& = & \frac{3 + 4}{10} \\
& = & 0.7
\end{eqnarray}$$

### 3. Precision
$$\begin{eqnarray}
Precision & = & \frac{True Positive}{True Positive + False Positive} \\
& = & \frac{3}{3 + 1} \\
& = & 0.75
\end{eqnarray}$$

### 4. Recall/Sensitivity
$$\begin{eqnarray}
Precision & = & \frac{True Positive}{True Positive + False Negative} \\
& = & \frac{3}{3 + 2} \\
& = & 0.6
\end{eqnarray}$$

## Exercise 03

### 1. Confusion Matrix
```{r, out.width = "50%"}
include_graphics("confusion-matrix-assignment06e03.jpg")
```

### 2. Accuracy
$$\begin{eqnarray}
Accuracy & = & \frac{True Positive + True Negative}{Total} \\
& = & \frac{7 + 1 + 2}{15} \\
& = & 0.67
\end{eqnarray}$$

## Exercise 04
In the first scenario, you would be able to achieve an accuracy of .51 by guessing 1 for all values (i.e. correctly predicting all observations that have 1 as their value).

In the second scenario, you would be able to achieve an accuracy of .99 by guessing 0 for every observation (i.e. correctly predicting all observations that have 0 as their value). 

Context matters for accuracy because it does not tell a complete story on its own. For example, your model may be very accurate at predicting one outcome, but terrible at another and this may not be obvious from the overall accuracy metric especially if there is high class imbalance. This is particularly important if you care more about the predictive accuracy for some outcomes than others. 

## Exercise 05

1. Divide the marbles data set into a training set with 80% of observations and a testing set 20% of observations. Set the seed to 20200229 before sampling.

```{r}

marbles_data <- read.csv("marbles.csv")

set.seed(20200229)

# create a split object
marbles_split <- initial_split(data = marbles_data, prop = 0.8)

# create the training and testing data
marbles_train <- training(x = marbles_split)
marbles_test <- testing(x = marbles_split)

```

2. Use count() and library(ggplot2) to develop and justify a intuitive/mental model for predicting black marbles.

```{r}

marbles_train %>% 
  count(color, size) %>% 
  ggplot() + 
  geom_col(mapping = aes(x = color, y = n, fill = size), position=position_dodge())

marbles_train %>% 
  count(color, size) %>% 
  ggplot() + 
  geom_col(mapping = aes(x = size, y = n, fill = color), position=position_dodge())

```

The simplest mental model for predicting black marbles is to predict that big marbles are black and small marbles are white. The majority of big marbles are black, and the majority of small marbles are white. 


3. Construct a custom function that takes a vector of sizes and returns a vector of predicted colors. Apply it to the testing data.

```{r}

predict_color <- function(.sizes) {
  predicted_colors <- c()
  for(i in 1:length(.sizes))
  {
    if(.sizes[i] == "big") {
      predicted_colors <- append(predicted_colors, "black")
    }
    else {
      predicted_colors <- append(predicted_colors, "white")
    }
  }
  return(predicted_colors)
}

predict_color(marbles_test$size)
```


4. Construct a custom function that takes y and y_hat that returns calculated accuracy and returns a confusion matrix. Use list() inside of return() to return more than one object. Apply it to the data from part 3. Do not use yardstick::conf_mat() or caret::confusionMatrix().

```{r}
cust_conf_matrix <- function(.y, .y_hat) {
  
  true_positive <- 0
  false_positive <- 0
  true_negative <- 0
  false_negative <- 0
  
  for(i in 1:length(.y))
  {
    if(.y[i] == "black" & .y_hat[i] == "black") {
      true_positive <- true_positive + 1
    }
    else if (.y[i] == "white" & .y_hat[i] == "black"){
      false_positive <- false_positive + 1
    }
    else if (.y[i] == "white" & .y_hat[i] == "white"){
      true_negative <- true_negative + 1
    }
    else if (.y[i] == "black" & .y_hat[i] == "white"){
      false_negative <- false_negative + 1
    }
  }
  
  predict_accuracy <- (true_positive + true_negative)/(true_positive + true_negative + false_negative + false_positive)
  
  conf_matrix <- matrix(c(true_positive, false_positive, false_negative, true_negative),ncol=2,byrow=TRUE)
  colnames(conf_matrix) <- c("y = black","y = white")
  rownames(conf_matrix) <- c("y_hat = black","y_hat = white")
  conf_matrix <- as.table(conf_matrix)
  
  return(list(predict_accuracy, conf_matrix))
  
}

predicted_colors <- predict_color(marbles_test$size)
cust_conf_matrix(marbles_test$color, predicted_colors)


```

5. Using the same testing and training data, estimate a decision tree/CART model with functions from library(parnsip). Use the “rpart” engine.

```{r}


# create the model object
predict_color_cart_mod <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

# fit the model
predict_color_cart_fit <- predict_color_cart_mod %>%
  fit(formula = as.factor(color) ~ ., data = marbles_train)

# visualize the decision tree
rpart.plot::rpart.plot(x = predict_color_cart_fit$fit)

```



6. Does the decision tree/CART model generate the same predictions on the testing data as the model from part 2? Why or why not?

```{r}

# use the model to get the predictions on the test data
color_predictions <- bind_cols(
  marbles_test,
  predict(object = predict_color_cart_fit, new_data = marbles_test)
)

# calculate the accuracy from the model predictions
accuracy(data = color_predictions,
         truth = as.factor(color),
         estimate = as.factor(.pred_class))

all.equal(as.factor(color_predictions$.pred_class), as.factor(predicted_colors))
```

Yes! The decision tree model produces the same predictions and the same accuracy on the testing data as the model from part 2 because it uses the same logic. Based on the data available, the best decision tree model available relied on the same test as the intuitive model: if the marble is big, predict black; if the marble is small, predict white.

## Exercise 06

```{r}

set.seed(20200302)

# input the data
rats <- tribble(
  ~rat_burrow, ~distance,
  1, 0.01,
  1, 0.05,
  1, 0.08,
  0, 0.1,
  0, 0.12,
  1, 0.2,
  1, 0.3,
  1, 0.5,
  1, 0.75,
  0, 0.9,
  1, 1,
  0, 1.2,
  0, 2.2,
  0, 2.3,
  0, 2.5,
  1, 3,
  0, 3.5,
  0, 4,
  0, 5,
  0, 7
) %>%
  mutate(rat_burrow = factor(rat_burrow))

#
split <- initial_split(rats, prop = 0.75)
rats_training <- training(split)
rats_testing <- testing(split)

rats_k1 <- vfold_cv(data = rats_training, 
                    v = 3)
rats_k3 <- vfold_cv(data = rats_training,
                    v = 3)
rats_kn <- vfold_cv(data = rats_training,
                    v = 3)
```

```{r}

# Extract analysis and assessment data from first resamples 
rats_k1_analysis1 <- training(rats_k1$splits[[1]])
rats_k1_assess1 <- testing(rats_k1$splits[[1]])

rats_k3_analysis1 <- training(rats_k3$splits[[1]])
rats_k3_assess1 <- testing(rats_k3$splits[[1]])

rats_kn_analysis1 <- training(rats_kn$splits[[1]])
rats_kn_assess1 <- testing(rats_kn$splits[[1]])

```

1. Calculate y_hat for the assessment data in the first resample of rats_k1 with k = 1.
```{r}
# This can be done by just looking at which observation is closest in the analysis data
# In the case of a tie (same distance from 2 observations in analysis) I chose randomly

# .01 closest to .05 -> 1
# .10 closest to .08 and .12, randomly choose .08 as closest -> 1
# .90 closest to 1.00 -> 1
# 2.30 closest to 2.20 -> 0
# 2.50 closes to 2.20 -> 0
y_hat_k1 <- data.frame("y_hat" = c(1, 1, 1, 0, 0))
rats_k1_assess1 <- bind_cols(rats_k1_assess1, y_hat_k1)
knitr::kable(rats_k1_assess1)
cust_conf_matrix(rats_k1_assess1$rat_burrow, rats_k1_assess1$y_hat)

```

2. Calculate y_hat for the assessment data in the first resample of rats_k3 with k = 3.
```{r}
# For this we'll want to find the three closest and use voting to determine the value of y_hat

# .01 closest to (1, 0.05), (1, .08), and (0, .12) -> 1
# .10 closest to (1, 0.05), (1, .08), and (0, .12) -> 1
# .20 closest to (1, 0.05), (1, .08), and (0, .12) -> 1
# .75 closest to (0, .90), (1, 1.00), and (0, 1.20) -> 0
# 3.50 closest to (0, 2.20), (1, 2.30), and (0, 2.50) -> 0
y_hat_k3 <- data.frame("y_hat" = c(1, 1, 1, 0, 0))
rats_k3_assess1 <- bind_cols(rats_k3_assess1, y_hat_k3)
knitr::kable(rats_k3_assess1)

```

3. Calculate y_hat for the assessment data in the first resample of rats_kn with k = n.
```{r}
# In this case, since k = n then y_hat is always the mode of y from the analysis data 

y_hat_kn <- data.frame("y_hat" = c(0, 0, 0, 0, 0))
rats_kn_assess1 <- bind_cols(rats_kn_assess1, y_hat_kn)
knitr::kable(rats_kn_assess1)

```

The k = n model was easiest to estimate computationally because the prediction was all the same since you were looking at all observations for each of them (i.e. the prediction was the mode every time). It was computationally the hardest to do k = 3 because you had to do more comparisons and calculations than for k = 1 where you could just compare with the closest observation. 

